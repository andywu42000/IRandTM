{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2415868251107653\n"
     ]
    }
   ],
   "source": [
    "import re, nltk, os, collections\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# R0672532\n",
    "DOCUMENTS_DIR = \"IRTM\"\n",
    "OUTPUT_DIR = \".\"\n",
    "VECTORFILE_DIR = \".\\Vector Files\"\n",
    "dictionary = {}\n",
    "seq_dict = {}\n",
    "all_terms = {}\n",
    "for filename in os.listdir(DOCUMENTS_DIR):\n",
    "    txt_dict = {}\n",
    "    txt_term = []\n",
    "    terms = ''\n",
    "    # 開啟news.txt檔並作初步的分割、去跳行及全小寫化\n",
    "    raw = open(os.path.join(DOCUMENTS_DIR, filename), 'r', encoding='UTF-8').read().lower().strip().split()\n",
    "\n",
    "    # token化分割的單詞\n",
    "    tokens = []\n",
    "    for text in raw:\n",
    "        tokens.append(re.compile(\"[^a-zA-Z]\").sub('', text))\n",
    "\n",
    "    # 使用nltk的PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    singles = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # 開啟stopwords.txt去除stopwords並輸出結果\n",
    "    stopwords = open(os.path.join(OUTPUT_DIR, 'stopwords.txt'), 'r', encoding='UTF-8').read()\n",
    "    filtered_words = [word for word in singles if word not in stopwords]\n",
    "\n",
    "    #將txt file每個term整理成dictionary並計算df\n",
    "    for word in filtered_words:\n",
    "        if(word not in dictionary):\n",
    "            dictionary.update({word: 1})\n",
    "        else:\n",
    "            dictionary[word] +=  1\n",
    "        terms+= (word + ' ')\n",
    "    od = collections.OrderedDict(sorted(dictionary.items(), key=lambda t: t[0]))\n",
    "    output_file2 = open(os.path.join(OUTPUT_DIR, 'dictionary.txt'), 'w', encoding='UTF-8')\n",
    "    output_file2.write('t_index\\tterm\\tdf\\n')\n",
    "    seq = 1\n",
    "    #輸出所有dictionary內的term及df\n",
    "    for k, v in od.items():\n",
    "        seq_dict.update({k: seq})\n",
    "        output_file2.write(str(seq) + '\\t' + k + '\\t' + str(v) + '\\n')\n",
    "        seq+=1\n",
    "    output_file2.close()\n",
    "    all_terms.update({str(filename):[terms]})\n",
    "    \n",
    "#使用scikit-learn計算tf-idf\n",
    "for filename in os.listdir(DOCUMENTS_DIR):\n",
    "    #將詞語換為矩陣\n",
    "    vectorizer=CountVectorizer()\n",
    "    #計算每個詞語的tf-idf\n",
    "    transformer=TfidfTransformer()\n",
    "    #前個fit_transform用於計算tf-idf，後個fit_transform將詞語換為矩陣\n",
    "    tfidf=transformer.fit_transform(vectorizer.fit_transform(all_terms[str(filename)]))\n",
    "    #獲取所有詞語\n",
    "    word=vectorizer.get_feature_names()\n",
    "    #將tf-idf轉為矩陣\n",
    "    weight=tfidf.toarray()\n",
    "    output_file3 = open(os.path.join(VECTORFILE_DIR, filename), 'w', encoding='UTF-8')\n",
    "    output_file3.write(str(len(word)) + '\\nt_index\\ttf-idf\\n')\n",
    "    #輸出結果\n",
    "    for i in range(len(weight)):\n",
    "        for j in range(len(word)):  \n",
    "            if(str(word[j]) in seq_dict):\n",
    "                output_file3.write(str(seq_dict[str(word[j])]) + '\\t' + str(weight[i][j]) + '\\n')\n",
    "            else:\n",
    "                output_file3.write('ERROR\\n')\n",
    "    output_file3.close()\n",
    "                \n",
    "#讀取docname的vector file並轉為矩陣\n",
    "def read_file_to_list(docname):\n",
    "    doc_content = open(os.path.join(VECTORFILE_DIR, docname), 'r', encoding='UTF-8').readlines()\n",
    "    doc_content = doc_content[2:]\n",
    "    result = []\n",
    "    for line in doc_content:\n",
    "        result.append(line.replace('\\n', '').split('\\t'))\n",
    "    return result\n",
    "\n",
    "#計算cosine similarity並print出結果\n",
    "def cosine(docx, docy):\n",
    "    docx_list = read_file_to_list(docx)\n",
    "    docy_list = read_file_to_list(docy)\n",
    "    cosine_sim_score = 0.0\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    \n",
    "    while (i < len(docx_list)) and (j < len(docy_list)):\n",
    "        x_t_index = int(docx_list[i][0])\n",
    "        y_t_index = int(docy_list[j][0])\n",
    "        x_tfidf = float(docx_list[i][1])\n",
    "        y_tfidf = float(docy_list[i][1])\n",
    "        \n",
    "        if x_t_index == y_t_index:\n",
    "            cosine_sim_score += x_tfidf * y_tfidf\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            if x_t_index > y_t_index:\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    print(cosine_sim_score)\n",
    "\n",
    "cosine('1.txt', '2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
